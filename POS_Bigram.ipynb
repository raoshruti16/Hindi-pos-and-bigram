{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package indian to\n",
      "[nltk_data]     C:\\Users\\dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package indian is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag import tnt\n",
    "from nltk.corpus import indian\n",
    "import nltk\n",
    "nltk.download('indian')\n",
    "from nltk.tree import Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hindi_model():\n",
    "    train_data = indian.tagged_sents('hindi.pos')\n",
    "    tnt_pos_tagger = tnt.TnT()\n",
    "    tnt_pos_tagger.train(train_data)\n",
    "    return tnt_pos_tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keywords(pos):\n",
    "    grammar = r\"\"\"NP:{<NN.*>}\"\"\"\n",
    "    chunkParser = nltk.RegexpParser(grammar)\n",
    "    chunked = chunkParser.parse(pos)\n",
    "    continuous_chunk = set()\n",
    "    current_chunk = []\n",
    "    for i in chunked:\n",
    "        if type(i) == Tree:\n",
    "            current_chunk.append(\" \".join([token for token, pos in i.leaves()]))\n",
    "        elif current_chunk:\n",
    "            named_entity = \" \".join(current_chunk)\n",
    "            if named_entity not in continuous_chunk:\n",
    "                continuous_chunk.add(named_entity)\n",
    "                current_chunk = []\n",
    "            else:\n",
    "                continue\n",
    "    return (continuous_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1=\"पाकिस्तान की पूर्व प्रधानमंत्री बेनजीर भुट्टो पर लगे भ्रष्टाचार के आरोप । पाकिस्तान की हाईकोर्ट के द्वारा बेनजीर की सुनवाई स्थगित ।\"\n",
    "text2=\"भारत के प्रधानमंत्री नरेंद्र मोदी हैं ।\"\n",
    "text3=\"साईंराज और समीर भाई है ।\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('पाकिस्तान', 'NNP'), ('की', 'PREP'), ('पूर्व', 'JJ'), ('प्रधानमंत्री', 'NN'), ('बेनजीर', 'NNPC'), ('भुट्टो', 'NNP'), ('पर', 'PREP'), ('लगे', 'VFM'), ('भ्रष्टाचार', 'NN'), ('के', 'PREP'), ('आरोप', 'NVB'), ('।', 'PUNC'), ('पाकिस्तान', 'NNP'), ('की', 'PREP'), ('हाईकोर्ट', 'NNPC'), ('के', 'PREP'), ('द्वारा', 'PREP'), ('बेनजीर', 'NNP'), ('की', 'PREP'), ('सुनवाई', 'NN'), ('स्थगित', 'JVB'), ('।', 'PUNC')]\n",
      "\n",
      "====KEYWORDS===\n",
      "{'प्रधानमंत्री बेनजीर भुट्टो', 'पाकिस्तान हाईकोर्ट', 'भ्रष्टाचार', 'पाकिस्तान', 'सुनवाई', 'बेनजीर'}\n"
     ]
    }
   ],
   "source": [
    "model = hindi_model()\n",
    "new_tagged = (model.tag(nltk.word_tokenize(text1)))\n",
    "print(new_tagged)\n",
    "print()\n",
    "print(\"====KEYWORDS===\")\n",
    "print(get_keywords(new_tagged))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('भारत', 'NNP'), ('के', 'PREP'), ('प्रधानमंत्री', 'NNC'), ('नरेंद्र', 'NNPC'), ('मोदी', 'NNP'), ('हैं', 'VFM'), ('।', 'PUNC')]\n",
      "\n",
      "====KEYWORDS===\n",
      "{'प्रधानमंत्री नरेंद्र मोदी', 'भारत'}\n"
     ]
    }
   ],
   "source": [
    "new_tagged = (model.tag(nltk.word_tokenize(text2)))\n",
    "print(new_tagged)\n",
    "print()\n",
    "print(\"====KEYWORDS===\")\n",
    "print(get_keywords(new_tagged))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('साईंराज', 'NNPC'), ('और', 'CC'), ('समीर', 'NNPC'), ('भाई', 'NN'), ('है', 'VFM'), ('।', 'PUNC')]\n",
      "\n",
      "====KEYWORDS===\n",
      "{'समीर भाई', 'साईंराज'}\n"
     ]
    }
   ],
   "source": [
    "new_tagged = (model.tag(nltk.word_tokenize(text3)))\n",
    "print(new_tagged)\n",
    "print()\n",
    "print(\"====KEYWORDS===\")\n",
    "print(get_keywords(new_tagged))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def build_conditional_probabilities(corpus):\n",
    " \n",
    "\n",
    "    tokenized_string = corpus.split()\n",
    "    previous_word = \"\"\n",
    "    dictionnary = defaultdict(list)\n",
    "\n",
    "    for current_word in tokenized_string:\n",
    "        if previous_word != \"\":\n",
    "            dictionnary[previous_word].append(current_word)\n",
    "        previous_word = current_word\n",
    "    \n",
    "\n",
    "\n",
    "    for key in dictionnary.keys():\n",
    "        next_words = dictionnary[key]\n",
    "        unique_words = set(next_words) # removes duplicated\n",
    "        nb_words = len(next_words)\n",
    "        probabilities_given_key = {}\n",
    "        for unique_word in unique_words:\n",
    "            probabilities_given_key[unique_word] = \\\n",
    "                float(next_words.count(unique_word)) / nb_words\n",
    "        dictionnary[key] = probabilities_given_key\n",
    "\n",
    "    return dictionnary\n",
    "\n",
    "\n",
    "def bigram_next_word_predictor(conditional_probabilities, current, next_candidate):\n",
    "\n",
    "\n",
    "    if current in conditional_probabilities:\n",
    "        if next_candidate in conditional_probabilities[current]:\n",
    "            return conditional_probabilities[current][next_candidate]\n",
    "    return 0.0\n",
    "\n",
    "# call the conditional probability dictionnary builder function\n",
    "conditional_probabilities = build_conditional_probabilities(text1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_next_word_predictor(conditional_probabilities, \"पाकिस्तान\",\"की\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_next_word_predictor(conditional_probabilities, \"की\", \"पूर्व\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_next_word_predictor(conditional_probabilities, \"के\", \"आरोप\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
